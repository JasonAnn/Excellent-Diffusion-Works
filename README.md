# Excellent-Diffusion-Works
Excellent Diffusion Works of top conferences &amp; Journals.

**CVPR2023**
> **More control for free! image synthesis with semantic diffusion guidance** [[paper](https://openaccess.thecvf.com/content/WACV2023/papers/Liu_More_Control_for_Free_Image_Synthesis_With_Semantic_Diffusion_Guidance_WACV_2023_paper.pdf)][[code]()][[project](xh-liu.github.io/sdg/)].
>
> **Paint by example: Exemplar-based image editing with diffusion models** [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf)][[code](https://github.com/Fantasy-Studio/Paint-by-Example.)].
>
> **Edict: Exact diffusion inversion via coupled transformations** [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Wallace_EDICT_Exact_Diffusion_Inversion_via_Coupled_Transformations_CVPR_2023_paper.pdf)][[code]()].
> 
> **ERNIE-ViLG 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts** [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf)][[code]()].


**arxiv**

> **Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models** [[paper](https://arxiv.org/pdf/2305.16322.pdf)][[code]()].
> 
> **Memory Efficient Diffusion Probabilistic Models via Patch-based Generation** [[paper](https://arxiv.org/pdf/2304.07087.pdf)][[code]()].
> 
> **Blended Latent Diffusion** [[paper](https://arxiv.org/pdf/2206.02779.pdf)][[code]()].
>
> **Designing an encoder for fast personalization of text-to-image models** [[paper](https://arxiv.org/pdf/2302.12228.pdf)][[code]()][[project](https://tuning-encoder.github.io/)].
>
> **Semantic Image Synthesis via Diffusion Models** [[paper](https://arxiv.org/pdf/2207.00050.pdf)][[code](https://github.com/WeilunWang/semantic-diffusion-model.)]
>
> **Generating coherent comic with rich story using ChatGPT and Stable Diffusion** [[paper](https://arxiv.org/pdf/2305.11067.pdf)][[code]()].
>
> **Extracting training data from diffusion models** [[paper](https://arxiv.org/pdf/2301.13188.pdf)][[code]()].
>
> **On enhancing the robustness of Vision Transformers: Defensive Diffusion** [[paper](https://arxiv.org/pdf/2305.08031.pdf)][[code]()].
>
>  **Easily accessible text-to-image generation amplifies demographic stereotypes at large scale** [[paper](https://arxiv.org/pdf/2211.03759.pdf?trk=public_post_comment-text)][[code]()].
>
> **Null-text inversion for editing real images using guided diffusion models**[[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf)][[code]()].



